= Netty

Netty 和 `ServerSocketChannel` 可以互相操作吗？

== JDK 原生 Socket 编程

使用 JDK 原生的 Socket API 进行网络编程：

[{java_src_attr}]
----
include::{sourcedir}/netty/ServerSocketTest.java[]
----

* 阻塞
* 高性能

== Server startup

[{java_src_attr}]
----
include::{sourcedir}/netty/Test03.java[]
----

两个问题

. 服务端的 Socket 在哪里初始化？
. 在哪里 accept 连接？

*Netty 服务端启动*

. 创建服务端 `Channel`
.. `io.netty.bootstrap.AbstractBootstrap.bind(int)`
.. `io.netty.bootstrap.AbstractBootstrap.doBind(SocketAddress)`
.. `io.netty.bootstrap.AbstractBootstrap.initAndRegister()`
. 初始化服务端 `Channel`
.. `NioServerSocketChannel.NioServerSocketChannel()` 在 `NioServerSocketChannel` 默认初始化函数中，使用了 JDK 内置的 `SelectorProvider.provider()` 方法返回的 `SelectorProvider` 对象。
.. 在 `NioServerSocketChannel.newSocket(SelectorProvider)` 方法中，调用 `provider.openServerSocketChannel()` 来创建 `ServerSocketChannel` 对象。
.. 在 `AbstractNioChannel.AbstractNioChannel(Channel, SelectableChannel, int)` 构造函数中，设置 `selectableChannel.configureBlocking(false)`。
. 注册 Selector
. 端口绑定 -- 调用底层 API，实现对端口的绑定。

*服务端 `Channel` 初始化过程*

. 通过 `bind()` 进入
. `initAndRegister()`
. 在 `AbstractBootstrap.initAndRegister()` 中通过 `channelFactory.newChannel()` 利用反射机制来创建 `Channel`。
. 在 `ServerBootstrap.init(Channel)` 中，初始化 `Channel`
.. `setChannelOptions`
.. `setAttributes`
.. 配置 `ChannelHandler` -- 配置服务端 pipeline。
... 初始化时，调用 `AbstractBootstrap.handler(io.netty.channel.ChannelHandler)`，配置 `ChannelHandler` 对象
... 通过调用 `AbstractBootstrap.initAndRegister()` 方法调用 `ServerBootstrap.init(Channel)` 方法，在其中，将 `ChannelHandler` 对象追加到 `Channel` 对象的 pipeline 的最后面。
.. add `ServerBootstrapAcceptor`
... 上一步执行完毕后，在 `ServerBootstrap.init(Channel)` 方法中，会创建一个 `ServerBootstrapAcceptor` 对象添加到 pipeline 后面。

*注册 selector*

. `AbstractChannel.AbstractUnsafe.register(EventLoop, ChannelPromise)` 入口
.. `AbstractChannel.this.eventLoop = eventLoop;` 绑定线程
.. `AbstractChannel.AbstractUnsafe.register0(ChannelPromise)` 实际注册
... `AbstractChannel.doRegister()` 调用底层 JDK API 注册
... `pipeline.invokeHandlerAddedIfNeeded()`
... `pipeline.fireChannelRegistered()`
+
从示例代码的输出可以看出，`Test03.ServerHandler` 中三个"事件"方法被调用的顺序是： `handlerAdded`，`channelRegistered` 和 `channelActive`。

*端口绑定*

. `AbstractChannel.AbstractUnsafe.bind(SocketAddress, ChannelPromise)` 入口
.. `AbstractBootstrap.doBind(SocketAddress)`
... `javaChannel().bind()` JDK 底层绑定 `io.netty.channel.AbstractChannel.AbstractUnsafe.bind`
.... ` pipeline.fireChannelActive()` 传播事件
... `HeadContext.readIfIsAutoRead()`


== `NioEventLoop`

// ch04

. 默认情况下，Netty 服务端起多少个线程？何时启动？
. Netty 是如何解决 JDK 空轮询 Bug 的？
. Netty 如何保证异步串行无锁化？

//

. `NioEventLoop` 创建
. `NioEventLoop` 启动
. `NioEventLoop` 执行逻辑

=== `NioEventLoop` 创建

`NioEventLoop` 默认是在调用 `NioEventLoopGroup()` 时被创建，默认是 2 倍的 CPU 数量（由常量 `MultithreadEventLoopGroup.DEFAULT_EVENT_LOOP_THREADS` 来定义）。

在 `MultithreadEventExecutorGroup.MultithreadEventExecutorGroup(int, Executor, EventExecutorChooserFactory, Object...)` 构造函数中：

. 创建 `new ThreadPerTaskExecutor(newDefaultThreadFactory())` 线程池；
+
每次执行任务都会创建一个线程实体。
+
`NioEventLoop` 线程命名规则 `nioEventLoop-1-XX`。在 `io.netty.util.concurrent.DefaultThreadFactory` 中可以看到。
+
这里还有两点需要注意：创建的线程对象和 `Runable` 被分别包装成了 `FastThreadLocalThread` 和 `FastThreadLocalRunnable`，主要是对 `ThreadLocal` 做了一些优化。
+
. 使用 `for` 循环，利用 `MultithreadEventExecutorGroup.newChild(Executor, Object...)` 方法创建 `NioEventLoop` 对象。
+
有三个作用：①保存线程执行器 `ThreadPerTaskExecutor`；②创建一个 `MpscQueue`；③创建一个 selector。
+
在 `NioEventLoop.newTaskQueue(int)` 方法，然后调用 `NioEventLoop.newTaskQueue0(int)` 方法，创建 `MpscQueue`。
+
. 调用 `DefaultEventExecutorChooserFactory.newChooser(EventExecutor[])` 方法，创建线程选择器。
+
`isPowerOfTwo()` 判断是否是 2 的幂，如果是则返回 `PowerOfTwoEventExecutorChooser`（优化），返回 `index++ & (length - 1)`；否则返回 `GenericEventExecutorChooser`（普通），返回 `abs(index++ % length)`。


=== `NioEventLoop` 启动

* 服务端启动绑定接口
* 新连接接入，通过 choose 绑定一个 `NioEventLoop`

在 `AbstractBootstrap.doBind0` 方法中，调用 `SingleThreadEventExecutor.execute(java.lang.Runnable)` 开始启动，再调用 `SingleThreadEventExecutor.execute(java.lang.Runnable, boolean)`，最后通过 `SingleThreadEventExecutor.startThread` 方法来启动。实际启动工作，最后委托给了 `SingleThreadEventExecutor.doStartThread` 方法来执行，这个方法中，调用 `SingleThreadEventExecutor.this.run();` 来启动 `NioEventLoop`。

=== `NioEventLoop` 执行逻辑

. `run() -> for(;;)`
.. `select()` 检查是否有 I/O 事件
.. `processSelectedKeys()` 处理 I/O 事件
.. `SingleThreadEventExecutor.runAllTasks(long)` 处理异步任务队列

*select() 方法*

* deadline 以及任务穿插逻辑处理
+
`NioEventLoop.select(long)`
+
* 阻塞式select
* 避免 JDK 空轮询的 Bug
+
在 `NioEventLoop.run()` 方法中，每次轮询，都会记录一下轮询次数 `selectCnt`；在 `NioEventLoop.unexpectedSelectorWakeup(selectCnt)`方法中，如果轮询次数大于 `SELECTOR_AUTO_REBUILD_THRESHOLD`(该值默认是 `512`，可以通过 `io.netty.selectorAutoRebuildThreshold` 参数来改)，则重建。
+
重建工作在 `NioEventLoop.rebuildSelector()` 方法中完成，然后又委托给 `NioEventLoop.rebuildSelector0()` 来实际执行。主要工作就是创建一个新的 `selector`，然后把老的 `selector` 上的 `SelectionKey` 注册到新的 `selector` 上。

*`processSelectedKeys()`*

* selected keySet 优化
+
`SelectedSelectionKeySet` 底层是一个数组。只实现了增加操作，删除操作没有实现。为什么？
+
* `processSelectedKeysOptimized()`
+
`NioEventLoop.processSelectedKeysOptimized()`，重点在 `NioEventLoop.processSelectedKey(SelectionKey, AbstractNioChannel)` 。





== Netty

创建 bossGroup 和 workerGroup
. 创建两个线程组 bossGroup 和 workerGroup
. bossGroup 只是处理连接请求，真正的客户端业务处理，会交给 workerGroup 完成
. 两个都是无限循环
. bossGroup 和 workerGroup 含有的子线程(NioEventLoop)的个数。默认 `CPU 内核数 * 2`，在 `io.netty.channel.MultithreadEventLoopGroup.DEFAULT_EVENT_LOOP_THREADS` 常量中定义

=== 异步任务

比如这里我们有一个非常耗时长的业务-> 异步执行 -> 提交该 channel 对应的 NIOEventLoop 的 taskQueue 中, 从 ctx -> pipeline -> eventLoop -> taskQueue 可以看到提交的任务。

.NettyServer
[{java_src_attr}]
----
include::{sourcedir}/netty/simple/NettyServer.java[]
----


.NettyServerHandler
[{java_src_attr}]
----
include::{sourcedir}/netty/simple/NettyServerHandler.java[]
----

.NettyClient
[{java_src_attr}]
----
include::{sourcedir}/netty/simple/NettyClient.java[]
----

.NettyClientHandler
[{java_src_attr}]
----
include::{sourcedir}/netty/simple/NettyClientHandler.java[]
----

Netty 的异步模型，就是基于 `ChannelFuture` 和 Listener 的监听回调模型。在入站、出站整个处理链上，可以注册各种各样的 Listener，以事件来驱动它们的调用。

=== HTTP

`io.netty.handler.codec.http.DefaultHttpRequest` 是 `io.netty.handler.codec.http.HttpObject` 的一个实现类。

为什么刷新一次浏览器，会有两个请求？ 浏览器增加了一次访问 ico 图标的请求。

HTTP 协议用完就关闭，所以，每次 pipeline 都不一样。跟 TCP 不太一样。


.TestServer
[{java_src_attr}]
----
include::{sourcedir}/netty/http/TestServer.java[]
----

.TestServerInitializer
[{java_src_attr}]
----
include::{sourcedir}/netty/http/TestServerInitializer.java[]
----

.TestServerHandler
[{java_src_attr}]
----
include::{sourcedir}/netty/http/TestServerHandler.java[]
----

[{java_src_attr}]
----

ServerBootstrap bootstrap = new ServerBootstrap();
bootstrap.group(bossGroup, workerGroup)
  .handler(null) // 添加到 bossGroup
  .childHandler(null); // 添加到 workerGroup
----

核心 API

. `Bootstrap`
. `ServerBootstrap`
. `Channel`
. `ChannelFuture`
. `ChannelHandler`
. `ChannelHandlerContext` -- `ChannelHandler` 中包含了一个 `ChannelHandlerContext`。
. `ChannelInboundHandler` 用于处理入站 I/O 事件
. `ChannelOutboundHandler` 用于处理出站 I/O 事件
//. `ChannelDuplexHandler` 用于处理入站和出站事件
. `ChannelPipeline` -- 一个重点。`ChannelPipeline` 是一个 Handler 的集合，它负责处理和拦截 inbound 或 outbound 的事件和操作，相当于一个贯穿 Netty 的链。


[plantuml,{diagram_attr}]
....
@startuml
skinparam nodesep 100
'skinparam ranksep 60

title <b>ChannelHandler 继承体系</b>

interface ChannelInboundHandler extends ChannelHandler
interface ChannelOutboundHandler extends ChannelHandler

abstract class ChannelHandlerAdapter implements ChannelHandler

class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler
class ChannelOutboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelOutboundHandler

@enduml
....

`ChannelHandler` 是一个很庞大的体系，也是 Netty 中非常核心的知识点。

在 Netty 中，每个 Channel 都有且仅有一个 `ChannelPipeline` 与之对应。

=== `ChannelHandlerContext`

`ChannelHandlerContext` 和 `ChannelHandler` 属于一对一关系。

=== `ChannelOption`

. `ChannelOption.SO_BACKLOG` -- 对应 TCP/IP 协议 `listen` 函数中的 `backlog` 参数，用于初始化服务器可连接队列大小。服务端处理客户端连接请求是顺序处理的，所以同一时间只能处理一个客户端连接。多个客户端来的时候，服务端将不能处理的客户端连接请求放在队列中等待处理，`backlog` 参数指定了队列大小。
. `ChannelOption.SO_KEEPALIVE` -- 一直保持连接活动状态。


可以从下面三个方面来学习网络编程

. 结合 epoll 源码，来说说 Java NIO 的实现。
. 了解 Linux 网络编程接口
. Java JDK 中对 Linux 网络编程接口的封装


`Unpooled` 操作缓冲区的。



`HttpObjectAggregator` 这个 Handler 可以实现报文聚合。怎么实现的？

在自定义 Handler 之前，经历过哪些 Handler？

`io.netty.handler.codec.ByteToMessageDecoder.decode` 这个方法会被反复调用，直到确定没有新的元素被添加到list，或者 `ByteBuf` 没有更多的可读字节为止。__这是怎么实现的？__


.`io.netty.handler.codec.MessageToByteEncoder.write`
[{java_src_attr},highlight=5..24]
----
@Override
public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception {
    ByteBuf buf = null;
    try {
        if (acceptOutboundMessage(msg)) {
            @SuppressWarnings("unchecked")
            I cast = (I) msg;
            buf = allocateBuffer(ctx, cast, preferDirect);
            try {
                encode(ctx, cast, buf);
            } finally {
                ReferenceCountUtil.release(cast);
            }

            if (buf.isReadable()) {
                ctx.write(buf, promise);
            } else {
                buf.release();
                ctx.write(Unpooled.EMPTY_BUFFER, promise);
            }
            buf = null;
        } else {
            ctx.write(msg, promise);
        }
    } catch (EncoderException e) {
        throw e;
    } catch (Throwable e) {
        throw new EncoderException(e);
    } finally {
        if (buf != null) {
            buf.release();
        }
    }
}
----

在 `ReplayingDecoder` 不需要判断数据是否足够读取，内部会进行处理判断。它是怎么实现的？

`ReplayingDecoder` 有如下问题：

. 并不是所有的 `ByteBuf` 操作都支持，如果调用了一个不支持的方法，就会抛出 `UnsupportedOperationException`
. 在某些情况下可能稍慢于 `ByteToMessageDecoder`，例如网络缓慢并且消息格式复杂时，消息会被拆成多个碎片，速度变慢。

有很多编解码器

. `LineBasedFrameDecoder`
. `DelimiterBasedFrameDecoder`
. `HttpObjectDecoder`
. `LengthFieldBasedFrameDecoder`

=== TCP 粘包和拆包

遇到异常就关闭连接，这样做合适吗？工程化的做法应该是怎么样的？如何重连或者重试？

粘包好奇怪，为什么第一次全部接受？后面的确开始不规则接受呢？

解决粘包问题的关键是解决服务器端每次读取数据长度的问题，这个问题解决，就不会出现服务器多读或者少读数据的问题，从而避免 TCP 粘包、拆包的问题。


== 源码分析

. `doBind` 方法
. `NioEventLoop` 中的 `run`


`io.netty.bootstrap.AbstractBootstrap.doBind` 建立 NIO 和 Netty 之间的联系。

`io.netty.channel.socket.nio.NioServerSocketChannel.doBind` 是一个重要的点。

执行到 `io.netty.channel.AbstractChannel.AbstractUnsafe.safeSetSuccess` 方法，就说明 promise 任务成功了。


`new NioEventLoopGroup()` 如果不指定参数，则默认创建的个数是内核数*2。

`io.netty.bootstrap.AbstractBootstrap.initAndRegister` 是源码分析的一个关键方法。

在 `io.netty.bootstrap.ServerBootstrap.init` 方法中完成了，`Channel` 和 `ChannelPipeline` 的关联。

`ChannelPipeline` 是一个双向链表，但是 `head` 和 `tail` 节点是空节点，添加和删除 Handler 都是在这两个节点之间进行。

调用 `ChannelPipeline` 的 `addLast` 方法增加 `ChannelHandler`，最后是在 `io.netty.channel.DefaultChannelPipeline.addLast(EventExecutorGroup, String, ChannelHandler)` 方法中，将 `ChannelHandler` 封装成了 `ChannelHandlerContext`，然后添加到 `ChannelPipeline` 链上的。

`io.netty.channel.nio.NioEventLoop.run` 方法中封装了事件轮询。

`io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages` 方法中，把 NIO 的 `SocketChannel` 封装成了 Netty 的 `NioSocketChannel` 对象。


== 自定义 Dubbo

[{java_src_attr}]
----
include::{sourcedir}/bytebuddy/ByteBuddyTest.java[]
----

[{java_src_attr}]
----
include::{sourcedir}/netty/buf/NettyByteBuf.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/chats/GroupChatClient.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/chats/GroupChatClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/chats/GroupChatServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/chats/GroupChatServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/consumer/DubboClientBootstrap.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/interfaces/HelloService.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/netty/NettyClient.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/netty/NettyClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/netty/NettyServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/netty/NettyServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/provider/DubboServerBootstrap.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/dubbo/provider/HelloServiceImpl.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/hearts/MyServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/hearts/MyServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/ByteToLongDecoder.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/ByteToLongDecoder2.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/IoClient.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/IoClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/IoClientInitializer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/IoServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/IoServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/IoServerInitializer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/iobound/LongToByteEncoder.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf/ClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf/ProtobufClient.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf/ProtobufServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf/ServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf/Student.proto[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf/StudentPOJO.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf2/ClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf2/MyDataInfo.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf2/ProtobufClient2.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf2/ProtobufServer2.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf2/ServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/protobuf2/Student.proto[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcp/TcpClient.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcp/TcpClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcp/TcpClientInitializer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcp/TcpServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcp/TcpServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcp/TcpServerInitializer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/MessageDecoder.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/MessageEncoder.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/MessageProtocol.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/TcpClientHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/TcpClientInitializer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/TcpServerHandler.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/TcpServerInitializer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/TcprotocolClient.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/tcprotocol/TcprotocolServer.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/ws/Server.java[]
----



[{java_src_attr}]
----
include::{sourcedir}/netty/ws/TextWebSocketFrameHandler.java[]
----



[{html_src_attr}]
----
include::{sourcedir}/netty/ws/hello.html[]
----







